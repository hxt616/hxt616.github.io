<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="参考：  李宏毅深度学习教程 Happy-LLM  总结构如下，被框住的部分一般都有N个   image-20250727221346536  自注意力机制 自注意力模型会“吃”整个序列的数据，输入几个向量，它就输出几个向量  对于输入序列中的每个 token（向量），我们会计算出三个矩阵：（X为输入矩阵即多个向量的组合，乘以对应的矩阵得到 Q、K、V）  Query (Q)：查询向量，\(Q &#x3D;">
<meta property="og:type" content="article">
<meta property="og:title" content="Transformer">
<meta property="og:url" content="http://example.com/2025/07/30/Transformer/index.html">
<meta property="og:site_name" content="hxt&#39;s Blog">
<meta property="og:description" content="参考：  李宏毅深度学习教程 Happy-LLM  总结构如下，被框住的部分一般都有N个   image-20250727221346536  自注意力机制 自注意力模型会“吃”整个序列的数据，输入几个向量，它就输出几个向量  对于输入序列中的每个 token（向量），我们会计算出三个矩阵：（X为输入矩阵即多个向量的组合，乘以对应的矩阵得到 Q、K、V）  Query (Q)：查询向量，\(Q &#x3D;">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/hxt616/PicGo@main/img/image-20250727221346536.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/hxt616/PicGo@main/img/image-20250727225641856.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/hxt616/PicGo@main/img/image-20250727230428672.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/hxt616/PicGo@main/img/image-20250727230452594.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/hxt616/PicGo@main/img/image-20250727220019098.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/hxt616/PicGo@main/img/image-20250727220057077.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/hxt616/PicGo@main/img/image-20250727221138994.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/hxt616/PicGo@main/img/image-20250727221042825.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/hxt616/PicGo@main/img/image-20250727221617793.png">
<meta property="og:image" content="https://raw.githubusercontent.com/datawhalechina/happy-llm/main/docs/images/2-figures/2-0.jpg">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/hxt616/PicGo@main/img/image-20250727224644292.png">
<meta property="article:published_time" content="2025-07-30T15:28:31.726Z">
<meta property="article:modified_time" content="2025-07-31T11:23:35.609Z">
<meta property="article:author" content="hxt">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="Transformer">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/hxt616/PicGo@main/img/image-20250727221346536.png">

<link rel="canonical" href="http://example.com/2025/07/30/Transformer/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Transformer | hxt's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>


<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">hxt's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-schedule">

    <a href="/schedule/" rel="section"><i class="fa fa-calendar fa-fw"></i>日程表</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/07/30/Transformer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/hxt.jpg">
      <meta itemprop="name" content="hxt">
      <meta itemprop="description" content="May the Force be with you.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="hxt's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Transformer
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-07-30 23:28:31" itemprop="dateCreated datePublished" datetime="2025-07-30T23:28:31+08:00">2025-07-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-07-31 19:23:35" itemprop="dateModified" datetime="2025-07-31T19:23:35+08:00">2025-07-31</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Transformer/" itemprop="url" rel="index"><span itemprop="name">Transformer</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" >
              <span class="post-meta-item-icon">
                <i class="eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>4.8k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>4 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>参考：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/datawhalechina/leedl-tutorial/releases/tag/v1.2.4">李宏毅深度学习教程</a></li>
<li><a target="_blank" rel="noopener" href="https://datawhalechina.github.io/happy-llm/#/">Happy-LLM</a></li>
</ul>
<p>总结构如下，被框住的部分一般都有N个</p>
<figure>
<img src="https://cdn.jsdelivr.net/gh/hxt616/PicGo@main/img/image-20250727221346536.png" alt="image-20250727221346536">
<figcaption aria-hidden="true">image-20250727221346536</figcaption>
</figure>
<h2 id="自注意力机制">自注意力机制</h2>
<p>自注意力模型会“吃”整个序列的数据，输入几个向量，它就输出几个向量</p>
<p><img src="https://cdn.jsdelivr.net/gh/hxt616/PicGo@main/img/image-20250727225641856.png" alt="image-20250727225641856" style="zoom:67%;"></p>
<p>对于输入序列中的每个
token（向量），我们会计算出三个矩阵：（X为输入矩阵即多个向量的组合，乘以对应的矩阵得到
Q、K、V）</p>
<ul>
<li><strong>Query (Q)</strong>：查询向量，<span class="math inline">\(Q
= W^QX\)</span></li>
<li><strong>Key (K)</strong>：键向量，<span class="math inline">\(K =
W^KX\)</span></li>
<li><strong>Value (V)</strong>：值向量，<span class="math inline">\(V =
W^VX\)</span></li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/hxt616/PicGo@main/img/image-20250727230428672.png" alt="image-20250727230428672" style="zoom:80%;"></p>
<p>计算注意力权重（softmax 后）： <span class="math display">\[
\text{Attention}(Q, K, V) =
\text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\]</span> <span class="math inline">\(d_k\)</span> 是 Key
向量的维度（用于缩放避免梯度过大）</p>
<blockquote>
<p><strong>多头注意力机制</strong>：</p>
<p>Transformer 实际使用的是
<strong>多头自注意力</strong>，先计算整个向量的
Q、K、V，再进行拆分，下面是完整步骤</p>
<p><strong>Step 1: 线性变换</strong></p>
<p>先通过三组权重矩阵，计算整个序列的 Q、K、V：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">python复制编辑Q = X @ W_Q  # [batch, seq_len, d_model] @ [d_model, d_model] -&gt; [batch, seq_len, d_model]</span><br><span class="line">K = X @ W_K</span><br><span class="line">V = X @ W_V</span><br></pre></td></tr></tbody></table></figure>
<p><strong>Step 2: 拆分成多头</strong></p>
<p>再把 Q/K/V 拆成多个头（通常是 reshape + transpose）：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 拆成多头，每一头的维度是 d_head</span></span><br><span class="line">Q = Q.reshape(batch_size, seq_len, num_heads, d_head).transpose(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># Q 形状变为 [batch, num_heads, seq_len, d_head]</span></span><br></pre></td></tr></tbody></table></figure>
<p><strong>Step 3: 每个头独立进行注意力计算</strong></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">attention_scores = softmax(Q @ K.transpose(-<span class="number">1</span>, -<span class="number">2</span>) / sqrt(d_head))  <span class="comment"># shape: [batch, num_heads, seq_len, seq_len]</span></span><br><span class="line">output_heads = attention_scores @ V  <span class="comment"># shape: [batch, num_heads, seq_len, d_head]</span></span><br></pre></td></tr></tbody></table></figure>
<p><strong>Step 4: 拼接所有头 &amp; 最终线性变换</strong></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 拼接所有头</span></span><br><span class="line">output = output_heads.transpose(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>).reshape(batch_size, seq_len, d_model)</span><br><span class="line">output = output @ W_O  <span class="comment"># 最终线性变换</span></span><br></pre></td></tr></tbody></table></figure>
<p><img src="https://cdn.jsdelivr.net/gh/hxt616/PicGo@main/img/image-20250727230452594.png" alt="image-20250727230452594" style="zoom:80%;"></p>
</blockquote>
<h2 id="编码器">编码器</h2>
<p>编码器输入一排向量，输出另外一排向量</p>
<p><img src="https://cdn.jsdelivr.net/gh/hxt616/PicGo@main/img/image-20250727220019098.png" alt="image-20250727220019098" style="zoom:50%;"></p>
<p>编码器内部可以分为多个块，如下图所示</p>
<p><img src="https://cdn.jsdelivr.net/gh/hxt616/PicGo@main/img/image-20250727220057077.png" alt="image-20250727220057077" style="zoom:67%;"></p>
<p>接下来就是每个块的细节</p>
<figure>
<img src="https://cdn.jsdelivr.net/gh/hxt616/PicGo@main/img/image-20250727221138994.png" alt="image-20250727221138994">
<figcaption aria-hidden="true">image-20250727221138994</figcaption>
</figure>
<p>这里使用层归一化，因为层归一化会在每个样本上计算其所有层的均值和方差，从而使每个样本的分布达到稳定。</p>
<h2 id="解码器">解码器</h2>
<h3 id="自回归解码器">自回归解码器</h3>
<p>在生成序列时，每一步仅基于<strong>当前及之前的token</strong>预测下一个token，这就要用到掩蔽自注意力（masked
self-attention），掩蔽自注意力可以通过一个掩码（mask）来阻止每个位置选择其后面的输入信息。</p>
<figure>
<img src="https://cdn.jsdelivr.net/gh/hxt616/PicGo@main/img/image-20250727221042825.png" alt="image-20250727221042825">
<figcaption aria-hidden="true">image-20250727221042825</figcaption>
</figure>
<h3 id="非自回归解码器">非自回归解码器</h3>
<p>是一种并行生成序列的模型，与自回归（AR）解码器逐步生成的方式不同，它一次性输出整个序列，显著提升了推理速度，但可能牺牲生成质量。</p>
<p>例如：给编码器一堆 <code>&lt;BOS&gt;</code>
的词元。假设输出的句子的长度有上限，绝对不会超过 300 个字。给编码器 300
个 <code>&lt;BOS&gt;</code>，就会输出 300 个字，输出
<code>&lt;EOS&gt;</code> 右边的的输出就当它没有输出。</p>
<figure>
<img src="https://cdn.jsdelivr.net/gh/hxt616/PicGo@main/img/image-20250727221617793.png" alt="image-20250727221617793">
<figcaption aria-hidden="true">image-20250727221617793</figcaption>
</figure>
<h2 id="编码器-解码器注意力">编码器-解码器注意力</h2>
<p>编码器的最终输出（即最后一层的隐藏状态）会作为<strong>Key和Value</strong>输入到解码器的<strong>每一层</strong>的交叉注意力（Cross-Attention）模块中。</p>
<p><img src="https://raw.githubusercontent.com/datawhalechina/happy-llm/main/docs/images/2-figures/2-0.jpg" style="zoom: 67%;"></p>
<p><img src="https://cdn.jsdelivr.net/gh/hxt616/PicGo@main/img/image-20250727224644292.png" alt="image-20250727224644292" style="zoom:80%;"></p>
<h2 id="流程总结">流程总结</h2>
<ol type="1">
<li><p><strong>输入预处理</strong></p>
<ul>
<li><p><strong>Tokenization</strong>：文本 → Token IDs（如
<code>[I, love, NLP]</code> →
<code>[1045, 2293, 4263]</code>）。</p></li>
<li><p><strong>Embedding</strong>：Token IDs →
词向量（<code>d_model</code> 维，如 512 维）。</p></li>
<li><p><strong>Positional Encoding</strong>：加入位置信息 → 最终输入向量
<code>X = WordEmbedding + PositionalEncoding</code>。</p>
<blockquote>
<p><strong>位置编码的区别</strong></p>
<ul>
<li><p><strong>Transformer（原始模型）</strong>：</p>
<ul>
<li><p>使用
<strong>固定的数学函数</strong>（正弦和余弦函数）生成位置编码，与输入嵌入（token
embeddings）<strong>相加</strong>。</p></li>
<li><p>公式： <span class="math display">\[
PE_{(pos, 2i)} =
\sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right), \quad
PE_{(pos, 2i+1)} =
\cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)
\]</span></p>
<p>其中 pos 是位置，i 是维度索引，d<sub>model</sub>
是模型维度。</p></li>
<li><p>这种编码是确定性的，无需学习，能捕捉绝对位置和相对位置关系。</p></li>
</ul></li>
<li><p><strong>BERT</strong>：</p>
<ul>
<li>使用 <strong>可学习的位置嵌入（Learned Positional
Embeddings）</strong>，即每个位置（如最大长度512）对应一个随机初始化的向量，通过训练更新。</li>
<li>位置嵌入与 token embeddings <strong>相加</strong>，类似
Transformer，但嵌入值是模型学到的，而非预设的数学函数</li>
</ul></li>
</ul>
</blockquote></li>
</ul></li>
<li><p><strong>Encoder 阶段（N 一般是 6 层）</strong></p>
<ul>
<li>每层执行：
<ul>
<li><strong>多头自注意力</strong>（计算全局依赖）</li>
<li><strong>前馈网络 FFN</strong>（非线性变换）</li>
<li><strong>残差连接 + LayerNorm</strong>（稳定训练）</li>
</ul></li>
<li>输出：上下文感知的编码表示 <code>Z</code>（供 Decoder 使用）。</li>
</ul></li>
<li><p><strong>Decoder 阶段（N 一般是 6 层）</strong></p>
<ul>
<li>每层执行：
<ul>
<li><strong>掩码自注意力</strong>（防止未来信息泄漏）</li>
<li><strong>残差连接 + LayerNorm</strong></li>
<li><strong>Encoder-Decoder 注意力</strong>（关注 <code>Z</code>）</li>
<li><strong>残差连接 + LayerNorm</strong></li>
<li><strong>前馈网络 FFN</strong></li>
<li><strong>残差连接 + LayerNorm</strong></li>
</ul></li>
<li>输出：解码后的隐藏状态 <code>H</code>。</li>
</ul></li>
<li><p><strong>输出生成</strong></p>
<ul>
<li><strong>线性层</strong>：将 <code>H</code> 投影到词表大小维度（如
50,000 维）。</li>
<li><strong>Softmax</strong>：生成概率分布 → 选择概率最高的 Token
作为输出（或采样）。</li>
</ul></li>
<li><p><strong>自回归（Autoregressive）</strong></p>
<ul>
<li>若为生成任务（如
GPT），将当前输出作为下一步输入，循环直至结束。</li>
</ul></li>
</ol>
<h2 id="bert和gpt">BERT和GPT</h2>
<ul>
<li><strong>BERT</strong>（Bidirectional Encoder Representations from
Transformers）——填空
<ul>
<li><strong>双向编码器</strong>：使用Transformer的<strong>Encoder</strong>部分，能同时利用上下文信息（左右双向上下文）。</li>
<li>适合理解型任务（如文本分类、实体识别、问答）。</li>
</ul></li>
<li><strong>GPT</strong>（Generative Pre-trained
Transformer）——预测下一个词
<ul>
<li><strong>单向解码器</strong>：使用Transformer的<strong>Decoder</strong>部分（掩码自注意力），没有交叉注意力（因为没有编码器的输出作为输入），仅依赖左侧上下文生成文本。</li>
<li>适合生成型任务（如文本生成、对话、续写）。</li>
</ul></li>
</ul>
<h2 id="post-norm-和-pre-norm">Post-Norm 和 Pre-Norm</h2>
<p>在 Transformer 及其变体模型中，<strong>Pre-Norm</strong> 和
<strong>Post-Norm</strong> 是两种不同的层归一化（Layer Normalization,
LayerNorm）放置方式，直接影响模型的训练稳定性、收敛速度和性能。以下是它们的详细对比：</p>
<h3 id="post-norm原始-transformer-设计"><strong>1. Post-Norm（原始
Transformer 设计）</strong></h3>
<ul>
<li><p><strong>结构顺序</strong>：</p>
<p><code>残差连接 → LayerNorm</code></p>
<p>即：<br>
<span class="math display">\[
\text{Output} = \text{LayerNorm}(x + \text{Sublayer}(x))
\]</span></p>
<ul>
<li>例如：多头注意力或前馈网络（FFN）的输出先与输入残差连接，再进行归一化。</li>
</ul></li>
<li><p><strong>特点</strong>：</p>
<ul>
<li><strong>原始 Transformer</strong>（论文中的编码器/解码器）和
<strong>GLM</strong>（由智谱开发的主流中文 LLM 之一）使用
Post-Norm。</li>
<li>训练初期可能不稳定（梯度方差大），需要配合 <strong>warm-up</strong>
学习率调度。</li>
<li>更深的模型容易因梯度消失/爆炸而难以训练。</li>
<li>理论上有更强的表达能力（因归一化在非线性变换后），但实际需要精细调参。</li>
</ul></li>
</ul>
<h3 id="pre-norm主流变体设计"><strong>2.
Pre-Norm（主流变体设计）</strong></h3>
<ul>
<li><p><strong>结构顺序</strong>：</p>
<p><code>LayerNorm → 残差连接</code></p>
<p>即： <span class="math display">\[
\text{Output} = x + \text{Sublayer}(\text{LayerNorm}(x))
\]</span></p>
<ul>
<li>先对输入归一化，再通过子层（如注意力或
FFN），最后与原始输入残差连接。</li>
</ul></li>
<li><p><strong>特点</strong>：</p>
<ul>
<li><strong>BERT、GPT 等现代模型普遍采用 Pre-Norm</strong>。</li>
<li>训练更稳定，无需 warm-up 也能快速收敛。</li>
<li>支持更深的网络（如 100+ 层），梯度传播更顺畅。</li>
<li>牺牲少量理论表达能力，换取实际训练的鲁棒性。</li>
</ul></li>
</ul>
<h3 id="核心区别对比"><strong>3. 核心区别对比</strong></h3>
<table>
<colgroup>
<col style="width: 21%">
<col style="width: 42%">
<col style="width: 36%">
</colgroup>
<thead>
<tr class="header">
<th>特性</th>
<th>Post-Norm</th>
<th>Pre-Norm</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>归一化位置</strong></td>
<td>残差连接后</td>
<td>残差连接前（子层输入）</td>
</tr>
<tr class="even">
<td><strong>训练稳定性</strong></td>
<td>依赖 warm-up，易不稳定</td>
<td>更稳定，适合深层模型</td>
</tr>
<tr class="odd">
<td><strong>梯度传播</strong></td>
<td>可能梯度消失/爆炸（深模型）</td>
<td>梯度更平缓，易于优化</td>
</tr>
<tr class="even">
<td><strong>表达能力</strong></td>
<td>理论上更强（非线性后归一化）</td>
<td>稍弱，但实际差距小</td>
</tr>
<tr class="odd">
<td><strong>典型应用</strong></td>
<td>原始 Transformer</td>
<td>BERT、GPT、T5 等现代模型</td>
</tr>
</tbody>
</table>
<h3 id="其他变体与改进"><strong>4. 其他变体与改进</strong></h3>
<ul>
<li><strong>ReZero Norm</strong>：将残差连接乘以可学习的标量（初始为
0），逐步增加非线性强度，结合两者优点。</li>
<li><strong>Sandwich Norm</strong>：在子层前后都加入 LayerNorm（如
<code>Norm → Sublayer → Norm → 残差</code>），进一步提升稳定性。</li>
<li><strong>DeepNorm</strong>（微软）：在 Post-Norm
基础上调整残差权重，用于极深模型（如 1000 层）。</li>
</ul>
<h2 id="llm">LLM</h2>
<p>三个阶段——Pretrain、SFT 和 RLHF</p>
<p>大语言模型（LLM）的训练通常分为三个关键阶段，每个阶段有不同的目标和方法：</p>
<h3 id="pretraining">1. Pretraining</h3>
<p>预训练</p>
<blockquote>
<p><strong>目标：</strong>
学习语言的通用知识（语法、语义、世界常识等）</p>
</blockquote>
<h4 id="过程">过程：</h4>
<ul>
<li>利用大规模通用语料（如网页、书籍、百科等）进行<strong>自监督学习</strong>。</li>
<li>训练任务通常是：
<ul>
<li><strong>因果语言建模（Causal LM）</strong>：预测下一个词（GPT
风格）</li>
<li><strong>掩码语言建模（Masked LM）</strong>：预测被遮盖的词（BERT
风格）</li>
</ul></li>
</ul>
<h4 id="特点">特点：</h4>
<ul>
<li>模型尚未特定任务对齐，仅有通用语言能力</li>
<li>参数量巨大（几十亿到千亿级别）</li>
<li>训练耗费算力与数据</li>
</ul>
<h3 id="sft">2. SFT</h3>
<p>Supervised Fine-Tuning，监督微调</p>
<blockquote>
<p><strong>目标：</strong>
将模型调整为能按照人类期望完成具体任务（如问答、摘要）</p>
</blockquote>
<h4 id="过程-1">过程：</h4>
<ul>
<li><p>使用<strong>人工标注的高质量指令-响应对</strong>进行监督微调（如
Alpaca、OpenAI InstructGPT 数据）</p></li>
<li><p>输入是类似：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">指令（Instruction）：请写一首关于春天的诗</span><br><span class="line">输出（Response）：春风吹绿柳，暖日照花枝……</span><br></pre></td></tr></tbody></table></figure></li>
</ul>
<h4 id="特点-1">特点：</h4>
<ul>
<li>显著提升对“任务”或“指令”的响应能力</li>
<li>强化了与人类沟通的能力，但输出可能仍存在幻觉或不可靠性</li>
</ul>
<blockquote>
<p>注意指令微调和 Prompt 工程的区别</p>
<table>
<colgroup>
<col style="width: 20%">
<col style="width: 37%">
<col style="width: 41%">
</colgroup>
<thead>
<tr class="header">
<th>项目</th>
<th>指令微调（Instruction Tuning）</th>
<th>Prompt 工程（Prompt Engineering）</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>是否训练模型</td>
<td>✅ 需要重新训练或微调模型</td>
<td>❌ 不训练，直接用</td>
</tr>
<tr class="even">
<td>成本</td>
<td>高：需要 GPU、数据、训练资源</td>
<td>低：只需要会写 prompt</td>
</tr>
<tr class="odd">
<td>是否修改模型参数</td>
<td>✅ 修改参数</td>
<td>❌ 不修改</td>
</tr>
<tr class="even">
<td>应用阶段</td>
<td>训练阶段（Finetune）</td>
<td>推理阶段（Inference）</td>
</tr>
<tr class="odd">
<td>优点</td>
<td>长期效果好、可扩展</td>
<td>灵活、快速试验</td>
</tr>
<tr class="even">
<td>缺点</td>
<td>昂贵、不易调试</td>
<td>不稳定、易受上下文影响</td>
</tr>
</tbody>
</table>
</blockquote>
<h3 id="rlhf">3. RLHF</h3>
<p>Reinforcement Learning with Human Feedback</p>
<blockquote>
<p><strong>目标：</strong>
利用人类偏好优化模型行为，提升对齐程度（alignment）</p>
</blockquote>
<h4 id="过程-2">过程：</h4>
<ol type="1">
<li><strong>收集人类偏好数据：</strong>
给出同一个输入，模型生成多个回答，由人类标注哪个更好。</li>
<li><strong>训练奖励模型（RM）：</strong>
学习如何从输入和输出中打分。</li>
<li><strong>强化学习（PPO 等）：</strong>
使用奖励模型对主模型进行强化学习优化。</li>
</ol>
<h4 id="特点-2">特点：</h4>
<ul>
<li>对齐人类偏好（例如：更礼貌、更安全、更有逻辑）</li>
<li>可以显著改善模型的对话体验和安全性（ChatGPT、Claude
等都采用该方式）</li>
</ul>
<h3 id="总结流程图">总结流程图</h3>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">原始语料（大规模网络文本等）</span><br><span class="line">        │</span><br><span class="line">        ▼</span><br><span class="line">    Pretraining（自监督）</span><br><span class="line">        │</span><br><span class="line">        ▼</span><br><span class="line">SFT（有标注任务数据，提升任务能力）</span><br><span class="line">        │</span><br><span class="line">        ▼</span><br><span class="line">RLHF（通过人类偏好进一步优化）</span><br></pre></td></tr></tbody></table></figure>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
              <a href="/tags/Transformer/" rel="tag"># Transformer</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2025/07/26/Go%20%E8%AF%AD%E8%A8%80%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86%E4%B8%8E%E5%8C%85%E7%AE%A1%E7%90%86/" rel="prev" title="Go语言项目管理与包管理">
      <i class="fa fa-chevron-left"></i> Go语言项目管理与包管理
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="nav-text">自注意力机制</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8"><span class="nav-text">编码器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A7%A3%E7%A0%81%E5%99%A8"><span class="nav-text">解码器</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%87%AA%E5%9B%9E%E5%BD%92%E8%A7%A3%E7%A0%81%E5%99%A8"><span class="nav-text">自回归解码器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9D%9E%E8%87%AA%E5%9B%9E%E5%BD%92%E8%A7%A3%E7%A0%81%E5%99%A8"><span class="nav-text">非自回归解码器</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8-%E8%A7%A3%E7%A0%81%E5%99%A8%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="nav-text">编码器-解码器注意力</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B5%81%E7%A8%8B%E6%80%BB%E7%BB%93"><span class="nav-text">流程总结</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#bert%E5%92%8Cgpt"><span class="nav-text">BERT和GPT</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#post-norm-%E5%92%8C-pre-norm"><span class="nav-text">Post-Norm 和 Pre-Norm</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#post-norm%E5%8E%9F%E5%A7%8B-transformer-%E8%AE%BE%E8%AE%A1"><span class="nav-text">1. Post-Norm（原始
Transformer 设计）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pre-norm%E4%B8%BB%E6%B5%81%E5%8F%98%E4%BD%93%E8%AE%BE%E8%AE%A1"><span class="nav-text">2.
Pre-Norm（主流变体设计）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E5%8C%BA%E5%88%AB%E5%AF%B9%E6%AF%94"><span class="nav-text">3. 核心区别对比</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B6%E4%BB%96%E5%8F%98%E4%BD%93%E4%B8%8E%E6%94%B9%E8%BF%9B"><span class="nav-text">4. 其他变体与改进</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#llm"><span class="nav-text">LLM</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#pretraining"><span class="nav-text">1. Pretraining</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BF%87%E7%A8%8B"><span class="nav-text">过程：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%89%B9%E7%82%B9"><span class="nav-text">特点：</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sft"><span class="nav-text">2. SFT</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BF%87%E7%A8%8B-1"><span class="nav-text">过程：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%89%B9%E7%82%B9-1"><span class="nav-text">特点：</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#rlhf"><span class="nav-text">3. RLHF</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BF%87%E7%A8%8B-2"><span class="nav-text">过程：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%89%B9%E7%82%B9-2"><span class="nav-text">特点：</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93%E6%B5%81%E7%A8%8B%E5%9B%BE"><span class="nav-text">总结流程图</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="hxt"
      src="/images/hxt.jpg">
  <p class="site-author-name" itemprop="name">hxt</p>
  <div class="site-description" itemprop="description">May the Force be with you.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">182</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">46</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">74</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">hxt</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">421k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">6:23</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
